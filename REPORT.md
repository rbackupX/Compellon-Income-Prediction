# Report
Overview of Problem: Binary Classification

## Table of Contents
- [Preprocessing](#preprocessing)
- [Model Considerations](#model-considerations)
- [Models](#models)
- [Evaluation Metrics](#evaluation-metrics)
- [Comparing Models](#comparing-models)
- [Conclusion](#conclusion)

## Preprocessing
- Missing Data
  * The most obvious shortcoming of this dataset is the presence of observations with missing fields. There are three primary ways in which such datasets can be handled. I could simply impute values into the missing fields or use an algorithm which was robust to incomplete observations. I elect instead to simply remove all these observations from the dataset. Of the 15,682 observations in the dataset, 996 have at least one missing covariate. Since only 6.35% of the observations are noisy, I just opted to remove them. There was no clear method for imputing the values, and while some of the models I use could be tolerant to incomplete observations, their implementations in Sci-kit Learn are not.  
- Aggregate Statistics
  * In the cleaned dataset, there are 14,686 observations, and 7508 are positively labeled (Income > 50k). There are 14 additional data points in each observation, which will be used to classify an observation. The dataset is also given such that all the positive examples come before all negative example, so I shuffle the dataframe after loading the data in the dataset. Only 6 of the 15 variables are given as integer values. Others, like Income and sex are easily converted into numeric representations. The remaining variables are categorical. 
- Univariate Analysis
  * The only integer variable that appears to be reasonably close to normally distributed is age. fnlwgt is right-skewed, so a log transformation of fnlwgt would be approximately normal. Because I later standardize features in some of my models, I elect not to log transform fnlwgt here because it will ultimately be normally distributed. Several of the categorical variables have a category with a definitive majority of the samples belonging to it, like Private in workplace, White in race, and United States in native-country. Conversely, there are several categories with so few members that the categories themselves are negligible (like Armed-Forces in occupation).
- Categorical Variables
  * There are several ways to encode categorical variables as numerical values, and various tactics are used in different scenarios in this assignment. The first method can be applied to categorical variables with two categories, like Income or sex. These can just be encoded as Indicator variables where I=1 if Income > 50k or sex=male and I=0 when Income <= 50 or sex=female. The next common approach is to take a categorical variable of with n categories, 0 to n-1, and encode it as n indicator variables where ni = 1 if the variable has category i. This poses two potential issues in our dataset. The first issue is that if we encode native-country as a categorical variable, the dimensionality of our model is going to blow up. One-hot encodings have this cost of increased dimensionality, so before performing the one-hot encoding, I group together categories when appropriate so that the subsequent dimensionality of my model is kept in check. The second possible issue is that introducing n one-hot variables for a categorical variable with n categories will result in perfect multicollinearity. I studied econometrics before I studied machine learning, and multicollinearity was discouraged because it could potentially mess with the coefficients in the model. In my ML classes, there's much less animus towards it. I'd argue this is because econometric models were more concerned with building explanatory models where each variables coefficient should be indicative of how it contributed to a prediction, whereas machine learning seems to be more focused on stable predictions. Thus, I tend to be permissive of some collinearity in ML models I build, but I always employ a collinearity-reducing measure from my econometrics classes when dealing with one-hot variables. If only n-1 variables are created to encode a categorical variable with n categories, we avoid multicollinearity. Pandas' get_dummies function has a parameter for omitting one variable when creating one hot variable encodings, but it doesn't guarantee that the category whose one-hot variable is dropped is the biggest category. I wrote my own function for creating one-hot variable encodings for this reason because dropping the variable for the largest category provides the greatest stability to the model. I'll discuss the final method of encoding categorical variables I used in the next section.
- Data Wrangling
  * As I previously noted, in an effort to reduce the dimensionality of my model, I grouped categories in the categorical variable native-country together before doing a one-hot encoding. I did this kind of category subsumption for several other categorical variables, like education, to reduce dimensionality. Not only were these grouping reducing the number of one-hot variables I needed to create, but they also allowed me to create features that enabled my model to generalize better. The main issue I struggled with when preparing my features was that the features from continuous-valued variables weren't realizing the expressive potential. Several of the variables were sparse in all but one interval and had high variance, like capital-gain. The difference between individuals who made $5000 and $9000 in capital gains last year appeared to my model to be significant, but the reality was that those two individuals both had very high probabilities of having incomes >50k. There were two remedies to this. The first was doing some form of standardization/scaling/normalization to reduce the magnitude of the distance between two such observations. Those methods still failed to capture this idea that observations that had a feature in a certain interval would likely have the same label. Thus, I opted to create two distinct data sets. The first dataset dataset_con only encodes categorical variables, leaving continuous ones alone (at least until a model opts to standardize/normalize its features down the road). The second dataset dataset_bin partitions each continuous value into some number of bins. Thus, each feature is initially labeled with an interval, rather than its observed continuous value. These intervals are then encoded, using the Sci-kit's LabelEncoder, which is analogous to ordinal encoding. Ordinal encoding differs from one-hot encoding in that it doesn't create n variables for a categorical variable with n categories; it just uses one variable. This variable has n distinct values, usually 1 - n, and category i of a categorical variable is assigned the value i in its ordinal-encoding variable. Ordinal encoding has benefits and drawbacks compared to one-hot encoding. It doesn't blow up the dimensionality of the feature space, which is good, and it contains another layer of meaning that is lost during one-hot encoding. A one-hot variable for some category has no notion of its relation to other categories in that categorical variable. Ordinal variables, on the other hand, have an implied tiered structure relative to the other categories in that categorical variable. Thus, when looking to encode a categorical variable, if the category has a natural hierarchy, then using ordinal encoding can preserve that meaning, whereas one-hot encoding will lose it. However, if no such ordering makes sense for a categorical variable, like occupation for example, then using ordinal encoding will inject arbitrary rank and impair your model's performance. The intervals of my continuous variables were placed in "bins" (categories) that do have a natural order to them. Therefore, using ordinal encoding for these variables makes sense and preserves information. To conclude my data-wrangling, I created one hybrid feature that was the product of age and hours-per-week (worked), which proved to be a highly impactful feature. 

## Model Considerations
- Basic Concerns
  * There are basic principles that should be considered when selecting a model, regardless of the dataset or application. Time and space complexity of a model and whether its parametric or nonparametric are basic considerations that should inform the beginning of any selection process. The bias-variance tradeoff always should be weighed; for a given model, increasing its dimensionality likely decreases at the expense of increased variance, so it's always a tradeoff. In this assignment, two issues I considered at the onset were overfitting and multicollinearity. The dataset wasn't large, but given the preprocessing I intended to do, the dimensionality could increase to the point that the model would overfit and not generealize. Additionally, I wanted to avoid large amounts of multicollinearity because it seemed obvious that some variables would definitely be correlated, like education and education-num. 
- Feature Selection
  * Feature selection was the next important part of creating my model. As previously mentioned, I felt that including all features would inevitably result in multicollinearity, and I wanted to limit multicollinearity for two reasons. First, having highly correlated covariates can make the model unstable. The other potential pitfall of multicollinearity is reducing explanatory power of the model. It can be hard to explain which covariates significantly impact the target if they're all highly correlated. The importance of having an easily-explained model is situation dependent, and in this situation, I felt predictive power was more important than explanatory power. The second issue I focused on during feature selection was the importance of each feature included. Part of the reason I am skeptical that my model has too much multicollinearity is that many of the highly correlated variables were ranked as unimportant. Between using the Random Forest Classifier to rank the importance of my variables and recurisve feature elimination to delineate which variables were included in my model, I felt confident about my feature selection. The recursive feature elimination algorithm, however, asserted that I should include every feature from my processed dataset in my actual model, so the dimensionality of my model is higher than expected. In my Classification notebook, you can visualize the correlation betweeen any two features in my model. There are only a few instances of significant correlation, like between relationship_wife and sex or age and age-hours, but these instances of correlation make sense and aren't frequent enough to cause concern about excessive multicollinearity amongst the features.  

## Models
- KNN 
  * Classification with a kNN-classifier can handle incomplete observations in the data, but the sci-kit learn implementation that I used is unable to do so. One of the key steps in data wrangling for a kNN is that every feature should be normalized. Like all distance-based methods, features with larger magnitude ranges can have a dominating impact on the distance function that the learner uses to label examples. These larger magnitudes likely don't correspond with an actual larger influence, as each feature may have in different units. Choosing the hyperparameters to tune during validation, and subsequently tuning them, was fairly basic. The only two hyperparameters I evaluated were k (obviously) and the distance function. Optimizing the hyperparameters had a minimal effect on the performance of the algorithm during testing, so I found acceptable values and moved on. Doing an exhaustive search of the hyperparameters state space using something like GridSearchCV barely improved on the hyperparameters I chose more haphazardly. Additionally, exhaustive cross-validation approaches made training the model quite slow. Indeed, a kNN classifier is quite slow to train and a parametric model with non-trivial space complexity, but it evaluates unlabeled examples efficiently.

- Random Forest Classifier
  * The Random Forest Classifier is an ensemble method composed of many predictors of the target. Because many predictors are trying to predict the same target, the Random Forest will outperform any single predictor alone. Random Forests leverage bagging to reduce the variance of their estimates, and they are capable of handling missing data. They can even be used for other parts of the ML pipeline; I use a Random Forest Classifier to rank feature importance prior to constructing any models. Tuning hyperparameters is more challenging for Random Forest Classifiers than kNN. They have many parameters that should be tuned to get a stable, well-behaved model, and many of those hyperparameters have a large range of possible values. Thus, I began tuning with RandomSearchCV to explore the hyperparameter space more efficiently, and once I found promising areas of the state space, I used GridSearchCV to more exhaustively find a local maxima in that region. Random Forest Classifiers can overfit, but this can be combatted by pruning and limiting the tree size (limiting the tree size can be enforced with some of its hyperparameters).  

- Logistic Regression
  * Logistic Regression is a common approach to binary classification, partially because of their good interpretability. Given an unlabeled example, the value the model returns is  its estimated probability of that example being labeled 1. It doesn't require a linear relationship between the features and the target, but it does require negligible multicollinearity between the features. When the covariates are highly correlated, logistic regression tends to overfit. Thus, when I introduce features like age-hours, I am hurting the performance of this model. To counteract such overfitting, regularization introduces a penalty term C that penalizes the model for having large weights (which are indicative of overfitting). Thus, given the moderate degree of multicollinearity in my dataset, I tuned two hyperparameters of my Logistic Regression model: C and the kind of regularization(L1/L2). 

## Evaluation Metrics
- Baseline
  * The most basic benchmark to set for the performance of the model is that it should predict the correct label for an unlabeled example with accuracy > frequency of the most common label (1 meaning Income >50k in our dataset) which = 0.51 
- Bayes Error Rate
  * Before attempting to optimize a model, I like to approximate an upper bound for the best possible accuracy I could achieve. I do this because if I create a model that approaches this approximated upper bound, I know to stop tuning my hyperparameters because the remaining error of my classifier is due to noise. This optimal error is the Bayes rate. Asymptotically the error rate of a 1-nearest-neighbor is less than twice the Bayes rate (asymptotically in the number of examples). 
- To evaluate each model, I use the following the following metrics: accuracy, precision, recall, and f1 score.

## Comparing Models
- KNN
  * The KNN classifier seems to be the worst of the three approaches. It requires the data to be cleaned (unlike RFC) to avoid covariates with large ranges overinfluencing the distance function. It is the only parametric model of the three, so the memory overhead is not insignificant. Theoretically, it can handle missing data, like the RFC, but the sklearn implementation cannot. While it was fairly simple to tune the hyperparameters, optimal tuning didn't seem to result in significant improvements in performance. It scored the lowest in accuracy, precision, recall, and f1 score for both the `dataset_bin.csv` and `dataset_con.csv` data sets.

- Random Forest Classifier
  * The Random Forest Classifier was the best classifier for this data set but has some downsides. The most obvious weaknesses of this model are that tuning it is both complicated and time-consuming. However, it is a parametric model and can evaluate unlabeled examples quickly. It can handle missing data and doesn't require the data to be cleaned. Because it uses bagging, and I disabled the hyperparameter that incorporates boosting, it is likely to have lower variance than Logistic Regression and won't overfit the data. In terms of performance, it performs slightly worse than logistic regression on the `dataset_bin.csv` data set in terms of accuracy and f1-score, but slightly better than logistic regression on `dataset_con.csv` for all four metrics used. 

- Logistic Regression
  * The Logistic Regression classifier is a solid classifier for this data set. It is a parametric model (unlike KNN), and its hyperparameters are easy to tune (unlike RFC). It is relatively fast both to train and to evaluate new examples. It requires more data cleaning than a RFC, as it expects standardized inputs, and it may slightly overfit this dataset as there are some covariates that exhibit a moderate degree of collinearity. It performs slightly better than the RFC on the `dataset_bin.csv` data set in accuracy and f1-score, but the improvements are slight.

- It is important to note that when I compare performances between different RFC and LR classifiers, I am comparing performances for that specific random permutation of the observations. I shuffled that observations during preprocessing because they were given in order with all the positive examples first. Given a different shuffling, the relative performances of the RFC and LR classifiers may differ. They are consistently very close to each other in all four evaluation metrics, and both consistently outperform the KNN classifier. 

## Conclusion
- I used many basic preprocessing steps to clean my data. The two datasets I created diverged in how they handled continuous values. `dataset_con.csv` simply preserved their values while `dataset_bin.csv` used binning and then ordinal encoding. When evaluating models using these two data sets, `dataset_con.csv` naturally slightly outperformed `dataset_bin.csv` because binning involves some information loss. The advantage of binning is that it can handle noisy observations more gracefully. 
- All three models used for my classifier were viable choices. KNN had more disadvantages than either RFC or LR, and ultimately, RFC seemed to be the best choice for this data set, as it avoided overfitting and could handle missing data.  
